{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting a Function with a Quantum Neural Network\n",
    "***\n",
    "Adapted from the original Xanadu Tutorial [here](https://pennylane.ai/qml/app/quantum_neural_net.html).\n",
    "\n",
    "In this example uses a variational quantum circuit to learn a fit for a one-dimensional function when being trained with noisy samples from that function.The variational circuit used is the continuous-variable quantum neural network model described [here](https://arxiv.org/abs/1806.06871)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is a Quantum Neural Network? (QNN)\n",
    "\n",
    "Broadly, a QNN is any quantum circuit with **trainable** (continuous) **parameters**. \n",
    "\n",
    "<img src=\"./images/qnn.png\" alt=\"QNN\" style=\"width: 1000px;\"/>\n",
    "\n",
    "\n",
    "We use qubits to physically represent the hidden layer nodes. In this case, our qubits are something called 'continuous-variable' photon states (each state or qubit is on a different 'rail' or 'photonic waveguide'). We can apply standard neural network transformations to these (weights, biases, and nonlinearities) through a physical modification of the photon state (e.g. weights = mixing through beamsplitters, bias = boosting the power, nonlinearity = performing an optical measurement). But unlike in classical neural networks, our nodes can be **entangled** or in **superposition**. Note that a N quantum nodes can encode 2^N classical nodes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import pennylane as qml\n",
    "from pennylane import numpy as np  # be sure to import numpy from pennylane itself!\n",
    "from pennylane.optimize import AdamOptimizer\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The device we use is the Strawberry Fields simulator, this time with only one quantum mode (or `wire`). You will need to have the Strawberry Fields plugin for PennyLane installed. Strawberry fields is a library for _emulating_ a quantum computer. Obviously this is not scalable beyond a small number of qubits (because that's the whole point of quantum computing... achieving compute capacity that classical computers cannot match). \n",
    "\n",
    "Note: the main innovation of PennyLane is automatic differentiation (i.e. computing gradients) on the quantum circuit. This is not a trivial task. Xanadu figured out a way to compute these gradients directly on the quantum hardware itself.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# using the \"Strawberry Fields\" quantum computer simulator as our device...\n",
    "device = qml.device(\"strawberryfields.fock\", wires=1, cutoff_dim=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our data will be a noisy sine curve. Let's generate one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_SAMPLES = 60\n",
    "NOISE_SPREAD = 0.15\n",
    "\n",
    "# seed the random function, for consistency across runs\n",
    "np.random.seed(0)\n",
    "\n",
    "# randomly sample across the x-axis\n",
    "X = np.random.uniform(-1, 1, NUM_SAMPLES)\n",
    "\n",
    "# compute corresponding y-values\n",
    "Y = np.sin(X*3.14159)\n",
    "\n",
    "# generate random noise for the y-axis\n",
    "noise = np.random.normal(0, NOISE_SPREAD, NUM_SAMPLES)\n",
    "Y = Y + noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting our data:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the data\n",
    "plt.figure()\n",
    "plt.scatter(X, Y)\n",
    "plt.xlabel(\"x\", fontsize=18)\n",
    "plt.ylabel(\"f(x)\", fontsize=18)\n",
    "plt.tick_params(axis=\"both\", which=\"major\", labelsize=16)\n",
    "plt.tick_params(axis=\"both\", which=\"minor\", labelsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantum Node\n",
    "\n",
    "Our QNN will be comprised of hidden layers having only a single qubit -- but remember that a quantum neuron can encode the equivalent of two classical neurons (it scales as $2^{N}$). For a single quantum node (qubit), the \"hidden layers\" of our QNN are defined as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def layer(v):\n",
    "    # Matrix multiplication of input layer (the weights)\n",
    "    qml.Rotation(v[0], wires=0)\n",
    "    qml.Squeezing(v[1], 0.0, wires=0)\n",
    "    qml.Rotation(v[2], wires=0)\n",
    "\n",
    "    # Bias\n",
    "    qml.Displacement(v[3], 0.0, wires=0)\n",
    "\n",
    "    # Element-wise nonlinear transformation\n",
    "    qml.Kerr(v[4], wires=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where the vector `v` represents our tunable parameters. \n",
    "\n",
    "Rotation, squeezing, displacement, and Kerr non-linearity are all quantum transformations (like \"gates\"). You don't need to worry about the details, only that they are equivalent to multiplication by weights, adding a bias, and applying our nonlinear activation function.\n",
    "\n",
    "We then build up our quantum neural net, where `var` is an array of tunable parameter vectors, one for each layer. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "@qml.qnode(device)\n",
    "def quantum_neural_net(var, x=None):\n",
    "    # Encode input x into quantum state\n",
    "    qml.Displacement(x, 0.0, wires=0)\n",
    "\n",
    "    # \"layer\" subcircuits\n",
    "    for v in var:\n",
    "        layer(v)\n",
    "\n",
    "    return qml.expval(qml.X(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " (A technical detail: the output is the expectation value of the 'x-quadrature', which is just a real-valued scalar number that represents the center of our quantum state along one dimension. Although our quantum state is multi-dimensional, we cannot read out all of these dimensions at once. We have to choose only one dimension to measure, and this is it, because the act of measurement 'collapses' the wavefunction and destroys the quantum state. Wavefunction collapse is simlar to Bayesian inference... when we learn something about the probability distribution, we change it; it's like going from the prior to the posterior distribution). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost Function\n",
    "\n",
    "The cost function we'll minimize is the square loss between target values (labels) and model predictions. Function fitting is a regression problem, and we interpret the expectations from the quantum node as predictions (i.e. without applying postprocessing such as thresholding).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def square_loss(labels, predictions):\n",
    "    loss = 0\n",
    "    for l, p in zip(labels, predictions):\n",
    "        loss = loss + (l - p) ** 2\n",
    "\n",
    "    loss = loss / len(labels)\n",
    "    return loss\n",
    "\n",
    "def cost(var, features, labels):\n",
    "    preds = [quantum_neural_net(var, x=x) for x in features]\n",
    "    return square_loss(labels, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "The network’s weights (called `var` here) are initialized with values sampled from a normal distribution. We use 4 layers; performance has\n",
    "been found to plateau at around 6 layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "NUM_LAYERS = 4\n",
    "\n",
    "np.random.seed(0)\n",
    "var_init = 0.05 * np.random.randn(NUM_LAYERS, 5)\n",
    "print(var_init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the Adam optimizer, we'll perform training to update our weights for 500 epochs (# passes over the training data). Grab some popcorn, this could take some time. Why? Because quantum systems are notoriously expensive to simulate on a classical computer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# execute this cell to begin training\n",
    "\n",
    "NUM_EPOCHS = 500\n",
    "\n",
    "opt = AdamOptimizer(0.01, beta1=0.9, beta2=0.999)\n",
    "\n",
    "cost_vals = []\n",
    "var = var_init\n",
    "for itr in range(NUM_EPOCHS):\n",
    "    var = opt.step(lambda v: cost(v, X, Y), var)\n",
    "    this_cost = cost(var, X, Y)\n",
    "    print(\"Iter: {:5d} | Cost: {:0.7f} \".format(itr + 1, this_cost))\n",
    "    cost_vals.append(this_cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's take a look at the loss function versus number of epochs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.scatter(np.linspace(0, NUM_EPOCHS, NUM_EPOCHS), cost_vals)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Cost\")\n",
    "plt.tick_params(axis=\"both\", which=\"major\")\n",
    "plt.tick_params(axis=\"both\", which=\"minor\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's use our model to generate some predictions, representing the function fitting, and plot this alongside our original datapoints and plot the shape of the function that the model has “learned” from the noisy data (green line)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_pred = np.linspace(-1, 1, 50)\n",
    "predictions = [quantum_neural_net(var, x=x_) for x_ in x_pred]\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(X, Y)\n",
    "plt.plot(x_pred, predictions, color=\"green\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"f(x)\")\n",
    "plt.tick_params(axis=\"both\", which=\"major\")\n",
    "plt.tick_params(axis=\"both\", which=\"minor\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model has learned to smooth the noisy data!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pennylane]",
   "language": "python",
   "name": "conda-env-pennylane-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
