{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Teaching an AI to Play Pong with Reinforcement Learning\n",
    "***\n",
    "\n",
    "### Intro\n",
    "\n",
    "* See the accompanying slide deck: [link to deck here]\n",
    "\n",
    "* This code and discussion borrows from Andrej Karpathy's excellent blog post, [\"Pong from Pixels\"](http://karpathy.github.io/2016/05/31/rl/).\n",
    "\n",
    "* If you want to take a step back and cover some basic RL concepts, [check out this blog post](https://blog.floydhub.com/an-introduction-to-q-learning-reinforcement-learning/).\n",
    "\n",
    "* There are several approaches to reinforcement learning (e.g. [some are described here](https://www.kdnuggets.com/2018/03/5-things-reinforcement-learning.html)). Below we're going to focus on the **policy gradient** approach.    \n",
    "\n",
    "* We use the [OpenAI gym library](http://gym.openai.com/docs/) for our Pong simulator. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gym[atari]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Training (Policy Gradient Approach)\n",
    "### Model Hyperparameters and Training Options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# *** MODEL HYPERPARAMS ***\n",
    "H = 200 # number of hidden layer neurons\n",
    "batch_size = 10 # number of episodes before a parameter update\n",
    "learning_rate = 1e-4\n",
    "gamma = 0.99 # discount factor for reward\n",
    "decay_rate = 0.99 # decay factor for RMSProp leaky sum of grad^2\n",
    "\n",
    "# *** TRAINING OPTIONS *** \n",
    "save_rate = 100 # save model after every 'x' episodes\n",
    "resume = False # resume from previous checkpoint?\n",
    "render = False\n",
    "\n",
    "# *** INITIALIZATION ***\n",
    "D = 80 * 80 # input dimensionality: 80x80 grid\n",
    "if resume:\n",
    "    model = pickle.load(open('save.p', 'rb'))\n",
    "else: \n",
    "    model = {}\n",
    "    # first layer is a matrix that maps 80*80 inputs (pixels) to H outputs (hidden units)\n",
    "    model['W1'] = np.random.randn(H,D) / np.sqrt(D) # \"Xavier\" initialization\n",
    "    # second layer here is just a vector of H weights (we'll apply a sigmoid to convert this into a probability)\n",
    "    model['W2'] = np.random.randn(H) / np.sqrt(H)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As per [Karpathy](http://karpathy.github.io/2016/05/31/rl/), we'll use a two-layer fully-connected net. Intuitively the first layer weights `W1` learn to recognize various game scenarios (e.g. ball at the top, our paddle in the middle) and the second layer weights `W2` can then decide in each case whether we should go UP or DOWN. Each path/line in the image below represents one tunable weight in our network. The nodes in this case represent us applying a non-linearity $f(x)$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./img/pong-net.png\" alt=\"Drawing\" style=\"width: 400px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mathematical Representation:\n",
    "\\begin{align*}\n",
    "h &= f(W_{1}X)\\\\\n",
    "logit &= W_{2}h \\\\\n",
    "P_{\\mathrm{up}} &= \\sigma (logit)\n",
    "\\end{align*}\n",
    "\n",
    "* $X$ is a flattened vector of input pixels\n",
    "* $f(x)$ is our non-linearity, which will be the [Rectified Linear Unit](https://machinelearningmastery.com/rectified-linear-activation-function-for-deep-learning-neural-networks/) (RELU), $f(x) = max(0, x)$\n",
    "* $\\sigma(x)$ is the [sigmoid function](https://www.sciencedirect.com/topics/computer-science/sigmoid-function), which maps its input to a probability between 0 and 1:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\sigma(x) = \\frac{e^{x}}{e^{x} + 1}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the outputs of the final layer (prior to the sigmoid) are often called logits, which is sometimes abbreviated as `logp`, not be confused with the mathematical expression log(P). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward Propagation\n",
    "Here's the code that evolves our network forward:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# *** convert our game pixels into a format that our neural network will understand\n",
    "def prepro(I):\n",
    "    \"\"\" preproccess 210x160x3 uint8 game frame into 6400 (80x80) 1D float vector \"\"\"\n",
    "    I = I[35:195] # crop\n",
    "    I = I[::2,::2,0] # downsample by factor of 2\n",
    "    I[I == 144] = 0 # erase background (background type 1)\n",
    "    I[I == 109] = 0 # erase background (background type 2)\n",
    "    I[I != 0] = 1 # everything else (paddles, ball) just set to 1\n",
    "    return I.astype(np.float).ravel()\n",
    "\n",
    "# *** helper function to convert our logits into probabilities\n",
    "def sigmoid(x): \n",
    "    return 1.0 / (1.0 + np.exp(-x)) # sigmoid \"squashing\" function to interval [0,1]\n",
    "\n",
    "# *** computing the probability of up, and the hidden states h, for this game frame\n",
    "def policy_forward(x):\n",
    "    h = np.dot(model['W1'], x)\n",
    "    h[h<0] = 0 # ReLU nonlinearity\n",
    "    logp = np.dot(model['W2'], h)\n",
    "    p = sigmoid(logp)\n",
    "    return p, h # return probability of taking action 2, and hidden state\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Back-Propagation\n",
    "\n",
    "Here's a recap of how our approach will work. For each game frame, our model computes the probability that it should move the paddle DOWN or UP. It rolls a dice and selects an outcome based on this probability, and we find out many turns later if that was a good move. \n",
    "\n",
    "Let the outcomes of a game state be represented by $y = 1 = \\mathrm{UP}$ and $y = 0 = \\mathrm{DOWN}$. We want to penalize bad moves and reward good moves. So how do we do this? Suppose, on a given game frame, the network chose to move the padel UP.  The probability of it making this decision is $P(y = 1 | X)$, \"probability of UP given X\". For future games, we want to either enhance this probability of UP, if it led to a good outcome, or discourage it if it led to a bad outcome. If we want to encourage it, we adjust our weights using the _gradient_ $\\partial L / \\partial W_{i}$, which tells us which direction to move the weights in to increase our probability of predicting UP. If we want instead to _discourage_ this move, we multiply the gradient by a negative factor, which moves the weights in the opposite direction. So, mathematically:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\Delta W_{i} = R \\left[\\frac{\\partial L}{\\partial W_{i}} \\right]\n",
    "\\end{equation*}\n",
    "\n",
    "* R is the reward factor (+1 for a win, -1 for a loss). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Policy Gradients** is this very idea of choosing an action, computing its gradient, and modulating that with the rewards from a later state. \n",
    "As Andrej Karpathy puts it:\n",
    "> *And thatâ€™s it: we have a stochastic policy that samples actions and then actions that happen to eventually lead to good outcomes get encouraged in the future, and actions taken that lead to bad outcomes get discouraged.* \n",
    "\n",
    "\n",
    "Where does the gradient come from? Our prediction problem can be represented as a case of binary classification, i.e. \"should I predict UP or DOWN for this input?\". Think of this part like a supervised learning task. We have X, and we want to predict the correct label y. We need a \"loss function\" that, when we minimize it, we are minimizing the error between the predicted and target labels. In this case we can use a variant of the [cross-entropy loss](http://neuralnetworksanddeeplearning.com/chap3.html) (see also [this](http://cs231n.github.io/neural-networks-2/#losses)):\n",
    "\\begin{equation*}\n",
    "L = y\\sigma(logit) + (1-y)(1 - \\sigma(logit))\n",
    "\\end{equation*}\n",
    "\n",
    "The gradients tell us how this loss changes as a function of the weights. We obtain these by taking partial derivates and using chain rule with good ole' Calculus:\n",
    "\\begin{align*}\n",
    "\\frac{\\partial L}{\\partial W_{2}} &= \\left(\\frac{\\partial L}{\\partial logit}\\right) \\left(\\frac{\\partial logit}{\\partial W_{2}}  \\right) \\\\\n",
    "\\frac{\\partial L}{\\partial W_{1}} &= \\left(\\frac{\\partial L}{\\partial logit}\\right) \\left(\\frac{\\partial logit}{\\partial h}  \\right) \\left(\\frac{\\partial h}{\\partial W_{1}}  \\right) \n",
    "\\end{align*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, \n",
    "\\begin{equation*}\n",
    "\\left(\\frac{\\partial L}{\\partial logit}\\right) = (y - \\sigma(h)), \\,\\,\\, \\left(\\frac{\\partial logit}{\\partial W_{2}}\\right) = h, \\,\\,\\, \\mathrm{so} \\frac{\\partial L}{\\partial W_{1}} =  (y - \\sigma(h)) \\cdot h\n",
    "\\end{equation*}\n",
    "\n",
    "The notation below can be a bit confusing, so here is the shorthand:\n",
    "* `dWi` = $\\frac{\\partial L}{\\partial W_{i}}$ are the gradients\n",
    "* `eph` are the hidden states `h` for this episode (ep)\n",
    "* `epdlogp` is dlogp = $\\frac{\\partial L}{\\partial logit}$ = $y - \\sigma(h)$ for this episode (ep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_backward(eph, epdlogp):\n",
    "    \"\"\" backward pass. (eph is array of intermediate hidden states for the episode) \"\"\"\n",
    "    dW2 = np.dot(eph.T, epdlogp).ravel()\n",
    "    dh = np.outer(epdlogp, model['W2'])\n",
    "    dh[eph <= 0] = 0 # backprop relu\n",
    "    dW1 = np.dot(dh.T, epx)\n",
    "    return {'W1':dW1, 'W2':dW2}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rewards\n",
    "\n",
    "If we win a game, should we encourage _every_ action that led to that win with a reward of +1? Not necessarily. The actions that happened in the distant past are probably not as important as the actions that immediately preceeded a win or a loss. \n",
    "\n",
    "So we typically use a 'discounted reward': The strength with which we encourage a sampled action is the weighted sum of all rewards afterwards, but later rewards are exponentially less important. \n",
    "\n",
    "\\begin{equation*}\n",
    "R_{t} = \\sum^{\\infty}_{k=0} \\gamma^{k}r_{t+k}\n",
    "\\end{equation*}\n",
    "* $\\gamma$ is the discount factor, a number between 0 and 1\n",
    "* $r_{t}$ is the reward for action $t$\n",
    "\n",
    "Note: we'll typically calculate this at the end of an epoch. So it has the effect of \"spreading\" the sparse rewards over the actions of that epoch... e.g. \n",
    "\\begin{equation*}\n",
    "[0, 0, 0, ...,  0, 0, 0, 0, 0, 0, 1] \\rightarrow [0, 0, 0, ... , 0.01, 0.06, 0.13, 0.38, 0.60, 0.83, 1.00]\n",
    "\\end{equation*}\n",
    "...where each element is the reward for a particular action in chronological sequence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discount_rewards(r):\n",
    "    \"\"\" take 1D float array of rewards and compute discounted reward \"\"\"\n",
    "    discounted_r = np.zeros_like(r)\n",
    "    running_add = 0\n",
    "    for t in reversed(range(0, r.size)):\n",
    "        if r[t] != 0: running_add = 0 # reset the sum, since this was a game boundary (pong specific!)\n",
    "        running_add = running_add * gamma + r[t]\n",
    "        discounted_r[t] = running_add\n",
    "    return discounted_r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Loop\n",
    "See comments for explanation. Note, we are interested in capturing the ball's motion, so we train on the difference between pixel frames. \n",
    "* For Pong, one episode is one complete set of games (an episode ends once one player has won 21 games). \n",
    "* For each episode, we have to track X, h, dlogp, rewards etc, for each game state (i.e. frame). \n",
    "* Note: for most frames, the reward will be zero, except for the last frame where it is +1 (win) or -1 (lose). \n",
    "  * Hence we must apply the discounted reward backwards through time.  \n",
    "* We accumulate our gradients over a batch of `batch_size` episodes, before applying weight updates to our parameters (this helps smooth our updates). \n",
    "* We control our weight updates with a learning rate using the [rmsprop method](https://towardsdatascience.com/a-look-at-gradient-descent-and-rmsprop-optimizers-f77d483ef08b) (also [this](https://towardsdatascience.com/understanding-rmsprop-faster-neural-network-learning-62e116fcf29a) and [this](http://ruder.io/optimizing-gradient-descent/)). \n",
    "\n",
    "It can take MANY hours for the performance to begin to improve substantially. Keep in mind, we're training here on regular CPUs, this can be sped up considerably by switching to GPUs (see Tensorflow implementation listed in the next section). You should start to see small improvements to the 'running mean' as the model trains. This might only be a few decimal points after the first several undred episodes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# *** INITIALIZE PONG ***\n",
    "env = gym.make(\"Pong-v0\")\n",
    "observation = env.reset()\n",
    "\n",
    "# *** INITIALIZE TRAINING VARIABLES ***\n",
    "prev_x = None # used in computing the difference frame\n",
    "xs,hs,dlogps,drs = [],[],[],[]\n",
    "running_reward = None\n",
    "reward_sum = 0\n",
    "episode_number = 0\n",
    "\n",
    "grad_buffer = { k : np.zeros_like(v) for k,v in model.items() } # update buffers that add up gradients over a batch\n",
    "rmsprop_cache = { k : np.zeros_like(v) for k,v in model.items() } # rmsprop memory\n",
    "\n",
    "history = {'episode_number': [], 'running_reward': [], 'reward_sum': []}\n",
    "\n",
    "# *** RUN TRAINING LOOP ***\n",
    "while True:\n",
    "    if render: env.render() # shows the simulated pong games in a pop-up\n",
    "\n",
    "    # --- preprocess the observation, set input to network to be difference image\n",
    "    cur_x = prepro(observation)\n",
    "    x = cur_x - prev_x if prev_x is not None else np.zeros(D)\n",
    "    prev_x = cur_x\n",
    "\n",
    "    # --- forward the policy network and sample an action from the returned probability\n",
    "    aprob, h = policy_forward(x)\n",
    "    action = 2 if np.random.uniform() < aprob else 3 # roll the dice!\n",
    "    # note: action of 2 signals UP, action of 3 signals down, in env.step(action)...\n",
    "    # ... these are like the specific game controller channels \n",
    "\n",
    "    # --- record various intermediates (needed later for backprop)\n",
    "    xs.append(x) # observation\n",
    "    hs.append(h) # hidden state\n",
    "    y = 1 if action == 2 else 0 # a \"fake label\" that matches the action\n",
    "    dlogps.append(y - aprob) # gradient that encourages the action that was taken to be taken \n",
    "\n",
    "    # -- step the environment and get new measurements\n",
    "    observation, reward, done, info = env.step(action)\n",
    "    reward_sum += reward\n",
    "    drs.append(reward) # record reward (has to be done after we call step() to get reward for previous action)\n",
    "\n",
    "    if done: # an episode finished\n",
    "        episode_number += 1\n",
    "\n",
    "        # --- stack together all inputs, hidden states, action gradients, and rewards for this episode\n",
    "        epx = np.vstack(xs) # inputs...\n",
    "        eph = np.vstack(hs) # hidden states...\n",
    "        epdlogp = np.vstack(dlogps) # action gradients\n",
    "        epr = np.vstack(drs) # rewards\n",
    "        xs,hs,dlogps,drs = [],[],[],[] # reset array memory\n",
    "\n",
    "        # --- compute the discounted reward backwards through time\n",
    "        discounted_epr = discount_rewards(epr)\n",
    "        # standardize the rewards to be unit normal (helps control the gradient estimator variance)\n",
    "        discounted_epr -= np.mean(discounted_epr)\n",
    "        discounted_epr /= np.std(discounted_epr)\n",
    "\n",
    "        # --- apply the discounted reward to the gradient \n",
    "        # note: mathematically, it doesn't matter if we compute the gradient first and then\n",
    "        # multiply it by R, or instead first multiply dL/dlogit by R and then compute gradient\n",
    "        epdlogp *= discounted_epr \n",
    "        grad = policy_backward(eph, epdlogp)\n",
    "        for k in model: grad_buffer[k] += grad[k] # accumulate gradient over batch\n",
    "\n",
    "        # --- perform rmsprop parameter update every batch_size episodes\n",
    "        if episode_number % batch_size == 0:\n",
    "            for k,v in model.items():\n",
    "                g = grad_buffer[k] # gradient\n",
    "                rmsprop_cache[k] = decay_rate * rmsprop_cache[k] + (1 - decay_rate) * g**2\n",
    "                model[k] += learning_rate * g / (np.sqrt(rmsprop_cache[k]) + 1e-5)\n",
    "                grad_buffer[k] = np.zeros_like(v) # reset batch gradient buffer\n",
    "\n",
    "        # --- book-keeping\n",
    "        running_reward = reward_sum if running_reward is None else running_reward * 0.9 + reward_sum * 0.1\n",
    "        print('resetting env. episode reward total was %f. running mean over past 10 episodes: %f' % (reward_sum, running_reward))\n",
    "        if episode_number % save_rate == 0: pickle.dump(model, open('saved_model.p', 'wb'))\n",
    "        history['episode_number'].append(episode_number)\n",
    "        history['running_reward'].append(running_reward)\n",
    "        history['reward_sum'].append(reward_sum)\n",
    "        if episode_number % save_rate == 0: pickle.dump(history, open('history.p', 'wb'))   \n",
    "        reward_sum = 0\n",
    "        observation = env.reset() # reset env\n",
    "        prev_x = None\n",
    "\n",
    "    if reward != 0: # Pong has either +1 or -1 reward exactly when game ends.\n",
    "        print ('ep %d: game finished, reward: %f' % (episode_number, reward) + ('' if reward == -1 else ' !!!!!!!!') )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### What to Expect\n",
    "For this model\n",
    "* Takes about 500 episodes to see if the mnodel is improving. \n",
    "* Takes about 8000 episodes to reach a point where the model is as good as computerized component (winning ~ half of the games). \n",
    "\n",
    "And here we've outlined a key challenge for reinforcement learning. Even though these networks can eventually surpass human performance, they're actually pretty poor learners compared to humans. We only need to play a couple games of pong to start to pick up on how to play it decently, whereas an AI needs to play thousands of games. \n",
    "\n",
    "Consider other learning tasks. When you first learn how to drive a car, you don't have to crash it a hundred times to know that you should stay on the road and in-between your lane lines. The reason is that you have other abstract models of your environment to guide you. Whereas an AI trained using vanilla Reinforcement Learning would fail many, many times before starting to succeed. \n",
    "\n",
    "RL still has a long way to go, but it shows promise when used in hybrid with other models, and may one day allow us to build more general-purpose AIs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Ways to Improve\n",
    "Building upon this basic implementation, we can achieve better performance, faster, with fewer training games:\n",
    "* Tensorflow implementation: https://github.com/mrahtz/tensorflow-rl-pong\n",
    "* Improve upon model using the Trust Region Policy Optimzation [(TRPO) method](https://arxiv.org/abs/1502.05477)\n",
    "* Add [L2 Regularization](https://towardsdatascience.com/intuitions-on-l1-and-l2-regularisation-235f2db4c261) (also see [this])(https://openreview.net/forum?id=HkGmDsR9YQ).\n",
    "\n",
    "* Another approach might be to create a simpler representation, i.e. give the network an inherent understanding of what a \"paddle\" or \"ball\" is. For example, we only need one vertical coordinate to track each paddle; for the ball we need its x-y coordinates as well as x-y velocities. Then instead of an 80x80 pixel input, we could only have 6 input features... "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Plot a Trained Model's History\n",
    "\n",
    "Note: an episode continues until one of the players reaches a score of 21. \n",
    "\n",
    "* A running reward of -21 means all games were lost. \n",
    "* Running reward of 0 means 50% of games were won. \n",
    "* Reward of 21 means all games were won. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "history = pickle.load(open('history.p', 'rb'))\n",
    "\n",
    "plt.plot(history['episode_number'], history['running_reward'])\n",
    "plt.title('Training History')\n",
    "plt.xlabel('Number of Training Episodes')\n",
    "plt.ylabel('Average Reward Per Episode')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Watching a Trained Model Play"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import pickle\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "model = pickle.load(open('trained_model.p', 'rb'))\n",
    "\n",
    "FRAME_REFRESH_RATE = 0.025 # in seconds\n",
    "\n",
    "D = 80 * 80 \n",
    "\n",
    "def prepro(I):\n",
    "    \"\"\" preproccess 210x160x3 uint8 game frame into 6400 (80x80) 1D float vector \"\"\"\n",
    "    I = I[35:195] # crop\n",
    "    I = I[::2,::2,0] # downsample by factor of 2\n",
    "    I[I == 144] = 0 # erase background (background type 1)\n",
    "    I[I == 109] = 0 # erase background (background type 2)\n",
    "    I[I != 0] = 1 # everything else (paddles, ball) just set to 1\n",
    "    return I.astype(np.float).ravel()\n",
    "\n",
    "def sigmoid(x): \n",
    "    return 1.0 / (1.0 + np.exp(-x)) # sigmoid \"squashing\" function to interval [0,1]\n",
    "\n",
    "def policy_forward(x):\n",
    "    h = np.dot(model['W1'], x)\n",
    "    h[h<0] = 0 # ReLU nonlinearity\n",
    "    logp = np.dot(model['W2'], h)\n",
    "    p = sigmoid(logp)\n",
    "    return p, h # return probability of taking action 2, and hidden state\n",
    "\n",
    "env = gym.make(\"Pong-v0\")\n",
    "observation = env.reset()\n",
    "prev_x = None # used in computing the difference frame\n",
    "\n",
    "while True:\n",
    "    env.render() \n",
    "    time.sleep(FRAME_REFRESH_RATE)\n",
    "\n",
    "    # --- preprocess the observation, set input to network to be difference image\n",
    "    cur_x = prepro(observation)\n",
    "    x = cur_x - prev_x if prev_x is not None else np.zeros(D)\n",
    "    prev_x = cur_x\n",
    "\n",
    "    # --- forward the policy network and sample an action from the returned probability\n",
    "    aprob, h = policy_forward(x)\n",
    "    action = 2 if np.random.uniform() < aprob else 3 # roll the dice!\n",
    "    y = 1 if action == 2 else 0 # a \"fake label\" that matches the action\n",
    "\n",
    "    # -- step the environment \n",
    "    observation, reward, done, info = env.step(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ai-pong]",
   "language": "python",
   "name": "conda-env-ai-pong-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
