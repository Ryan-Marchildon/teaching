{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sound Classification with CNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook walks through the process of building a simple CNN-based network for classifying 10 different urban sounds. \n",
    "\n",
    "We'll be using the UrbanSound8k dataset. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Obtaining the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-ZgvcSpS4W6v"
   },
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import tarfile\n",
    "import os\n",
    "\n",
    "if not os.path.exists(os.path.join(os.getcwd(), 'UrbanSound8K')):\n",
    "    urllib.request.urlretrieve(\"https://zenodo.org/record/1203745/files/UrbanSound8K.tar.gz\",\"a.tar.gz\")\n",
    "    tar = tarfile.open(\"a.tar.gz\")\n",
    "    tar.extractall()\n",
    "    tar.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Feature Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AUHre7lQ4eAF"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from librosa import display\n",
    "import librosa\n",
    "import IPython.display\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the Metadata File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VPLIlUJN4euz"
   },
   "outputs": [],
   "source": [
    "# forming a panda dataframe from the metadata file\n",
    "data = pd.read_csv(\"UrbanSound8K/metadata/UrbanSound8K.csv\")\n",
    "\n",
    "# display the first 5 rows of this dataframe\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of files in each class\n",
    "data[\"class\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AXnks9vV4kct"
   },
   "outputs": [],
   "source": [
    "# number of files in each folder\n",
    "data[\"fold\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import an Audio Clip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this file is of a dog bark\n",
    "y, sr = librosa.load(\"UrbanSound8K/audio/fold5/100032-3-0-0.wav\")\n",
    "\n",
    "# display the waveform\n",
    "librosa.display.waveplot(y=y, sr=sr)\n",
    "\n",
    "# play it back\n",
    "IPython.display.Audio(data=y, rate=sr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "PrpVl_uJFtRz",
    "outputId": "72ac220b-c616-4bab-e040-b1f1d6be20a5"
   },
   "outputs": [],
   "source": [
    "# let's extract some audio features... \n",
    "NUM_MEL_BINS = 40\n",
    "\n",
    "mfccs = librosa.feature.mfcc(y, sr, n_mfcc=NUM_MEL_BINS)\n",
    "melspectrogram = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=NUM_MEL_BINS, fmax=8000)\n",
    "# chroma_stft = librosa.feature.chroma_stft(y=y, sr=sr, n_chroma=NUM_FEATURE_BINS)\n",
    "# chroma_cq = librosa.feature.chroma_cqt(y=y, sr=sr, n_chroma=NUM_FEATURE_BINS)\n",
    "# chroma_cens = librosa.feature.chroma_cens(y=y, sr=sr, n_chroma=NUM_FEATURE_BINS)\n",
    "\n",
    "# display dimensions\n",
    "print('Number of mel filter banks:', melspectrogram.shape[0])\n",
    "print('Number of analysis windows (time frames):', melspectrogram.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 293
    },
    "colab_type": "code",
    "id": "MfAip77-F78F",
    "outputId": "1fd581e0-cad3-4444-ff81-91e749de3316"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# mel-frequency cepstral coefficients (mfccs)\n",
    "plt.figure(figsize=(10,4))\n",
    "librosa.display.specshow(mfccs, x_axis='time')\n",
    "plt.colorbar()\n",
    "plt.title('MFCC')\n",
    "plt.tight_layout()\n",
    "\n",
    "# Mel spectrogram\n",
    "plt.figure(figsize=(10,4))\n",
    "librosa.display.specshow(librosa.power_to_db(melspectrogram,ref=np.max),y_axis='mel', fmax=8000,x_axis='time')\n",
    "plt.colorbar(format='%+2.0f dB')\n",
    "plt.title('Mel spectrogram')\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "b4xPM6yi4nxW"
   },
   "outputs": [],
   "source": [
    "NUM_MEL_BINS = 40\n",
    "FEATURE = 'mfccs'  # 'melspect' or 'mfccs'\n",
    "PATH = \"UrbanSound8K/audio/fold\"\n",
    "\n",
    "X_train = []\n",
    "X_test = []\n",
    "y_train = []\n",
    "y_test = []\n",
    "\n",
    "for i in tqdm(range(len(data))):\n",
    "    \n",
    "    fold_no = str(data.iloc[i][\"fold\"])\n",
    "    file = data.iloc[i][\"slice_file_name\"]\n",
    "    label = data.iloc[i][\"classID\"]\n",
    "    \n",
    "    filepath = PATH + fold_no + \"/\" + file\n",
    "    \n",
    "    y, sr = librosa.load(filepath)\n",
    "    \n",
    "    if FEATURE is 'melspect':\n",
    "        feature_array = librosa.feature.mfcc(y, sr, n_mfcc=NUM_MEL_BINS)\n",
    "    elif FEATURE is 'mfccs':\n",
    "        feature_array = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=NUM_MEL_BINS, fmax=8000)\n",
    "    else:\n",
    "        raise ValueError('FEATURE must be specified as \"mfccs\" or \"melspect\"')\n",
    "        \n",
    "    if(fold_no != '10'):\n",
    "        X_train.append(feature_array)\n",
    "        y_train.append(label)\n",
    "    else:\n",
    "        X_test.append(feature_array)\n",
    "        y_test.append(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Extracted Features to Disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURES_DIR = os.path.join(os.getcwd(), 'features')\n",
    "if not os.path.exists(FEATURES_DIR):\n",
    "    os.mkdir(FEATURES_DIR)\n",
    "\n",
    "save_dir = os.path.join(FEATURES_DIR, FEATURE)\n",
    "if not os.path.exists(save_dir):\n",
    "    os.mkdir(save_dir)\n",
    "\n",
    "with open(os.path.join(save_dir, 'X_train.pkl'), 'wb') as f:\n",
    "    pickle.dump(X_train, f)\n",
    "with open(os.path.join(save_dir, 'X_test.pkl'), 'wb') as f:\n",
    "    pickle.dump(X_test, f)\n",
    "with open(os.path.join(save_dir, 'y_train.pkl'), 'wb') as f:\n",
    "    pickle.dump(y_train, f)\n",
    "with open(os.path.join(save_dir, 'y_test.pkl'), 'wb') as f:\n",
    "    pickle.dump(y_test, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Extracted Features from Disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURE = 'melspect'  # 'melspect' or 'mfccs'\n",
    "\n",
    "load_dir = os.path.join(os.getcwd(), 'features', FEATURE)\n",
    "\n",
    "with open(os.path.join(load_dir, 'X_train.pkl'), 'rb') as f:\n",
    "    X_train = pickle.load(f)\n",
    "with open(os.path.join(load_dir, 'X_test.pkl'), 'rb') as f:\n",
    "    X_test = pickle.load(f)\n",
    "with open(os.path.join(load_dir, 'y_train.pkl'), 'rb') as f:\n",
    "    y_train = pickle.load(f)\n",
    "with open(os.path.join(load_dir, 'y_test.pkl'), 'rb') as f:\n",
    "    y_test = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enforce Common Window Size on Feature Data\n",
    "\n",
    "To train a CNN, we need to have an equal number of time steps from each sample. That's not the case with this dataset. But most (83%) have 173 time steps. Rather than pad samples that have too few, or trim samples that have too many, as a shortcut here I'll only use samples with 173 times steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# examining the distribution of time steps per sample...\n",
    "sizes_train = [sample.shape[1] for sample in X_train]\n",
    "sizes_test = [sample.shape[1] for sample in X_test]\n",
    "\n",
    "plt.hist(np.array(sizes_train), bins=20)\n",
    "plt.ylabel('No of times')\n",
    "plt.show()\n",
    "\n",
    "print('Fraction of samples in X_train having 173 time steps:', sizes_train.count(173)/len(sizes_train))\n",
    "print('Fraction of samples in X_test having 173 time steps:', sizes_test.count(173)/len(sizes_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# restricting the samples\n",
    "def restrict_samples(X_data, y_data):\n",
    "    \n",
    "    X = []\n",
    "    y = []\n",
    "    \n",
    "    for idx in range(len(X_data)):\n",
    "        if X_data[idx].shape[1] == 173:\n",
    "            X.append(X_data[idx])\n",
    "            y.append(y_data[idx])\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "X_train, y_train = restrict_samples(X_train, y_train)\n",
    "X_test, y_test = restrict_samples(X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should double-check that this hasn't significantly changed the balance of samples among our classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dictionary for mapping numerical class labels to the class name\n",
    "class_label_to_name = {\n",
    "    0 : 'air_conditioner',\n",
    "    1 : 'car_horn',\n",
    "    2 : 'children_playing',\n",
    "    3 : 'dog_bark',\n",
    "    4 : 'drilling',\n",
    "    5 : 'engine_idling',\n",
    "    6 : 'gun_shot',\n",
    "    7 : 'jackhammer',\n",
    "    8 : 'siren',\n",
    "    9 : 'street_music'\n",
    "}\n",
    "\n",
    "def map_labels_to_names(labels):\n",
    "    label_names = [class_label_to_name[label] for label in labels]\n",
    "    return label_names\n",
    "\n",
    "y_train_names = map_labels_to_names(y_train)\n",
    "y_test_names = map_labels_to_names(y_test)\n",
    "\n",
    "df_train_names = pd.DataFrame({'class_names': y_train_names}) \n",
    "df_test_names = pd.DataFrame({'class_names': y_test_names}) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Class count within restricted training dataset:')\n",
    "df_train_names[\"class_names\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Class count within restricted test dataset:')\n",
    "df_test_names[\"class_names\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-Hot Encoding of Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "UDbtZ-GaXEXS",
    "outputId": "50df72f3-2b7e-45b7-ea23-80e5aaca1743"
   },
   "outputs": [],
   "source": [
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "# convert from lists to arrays\n",
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "# apply one-hot encoding\n",
    "y_train = to_categorical(y_train, num_classes=10)\n",
    "y_test = to_categorical(y_test, num_classes=10)\n",
    "\n",
    "print('Shape of y_train:', y_train.shape)\n",
    "print('Shape of y_test:', y_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reshape Feature Data\n",
    "The CNN expects an additional \"channels\" dimension. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "CgeQAvcdgtwn",
    "outputId": "fb3d635b-1da0-4c3d-f17f-50c9619cf9da"
   },
   "outputs": [],
   "source": [
    "# convert to arrays (fails unless you enforced equal number of time steps)\n",
    "X_train = np.array(X_train)\n",
    "X_test = np.array(X_test)\n",
    "print('Shape of X_train (before):', X_train.shape)\n",
    "\n",
    "# add an additional dimension (note: we only have 1 channel here)\n",
    "X_train = np.expand_dims(X_train, axis=-1)\n",
    "X_test= np.expand_dims(X_test, axis=-1)\n",
    "print('Shape of X_train (after):', X_train.shape)\n",
    "print('(num samples, num mel bins, num time steps, num channels)')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardize the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = np.mean(X_train.flatten(), axis=0) \n",
    "std = np.std(X_train.flatten(), axis=0)\n",
    "\n",
    "X_train = (X_train - mean)/std\n",
    "X_test = (X_test - mean)/std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## ML Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras as Keras\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import (\n",
    "    Conv2D, MaxPooling2D, BatchNormalization,\n",
    "    Dropout, Dense, Activation, Input, concatenate,\n",
    "    Flatten)\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.regularizers import l1, l2\n",
    "\n",
    "print('TensorFlow Version:', tf.__version__)\n",
    "print('Keras Version:', Keras.__version__)\n",
    "print('Keras Backend:', Keras.backend.backend())\n",
    "print('Keras Conv Data Format:', Keras.backend.image_data_format())\n",
    "\n",
    "print(\"Is GPU available?:\", tf.test.is_gpu_available())\n",
    "print(\"GPU name:\", tf.test.gpu_device_name())\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "\n",
    "# config = tf.ConfigProto()\n",
    "# config.gpu_options.allow_growth = True\n",
    "# sess = tf.Session(config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fUJUiyVTgkGZ"
   },
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# first conv stack\n",
    "model.add(Conv2D(\n",
    "    filters=64, \n",
    "    kernel_size=5, \n",
    "    strides=1, \n",
    "    padding=\"same\",\n",
    "    activation=\"relu\",\n",
    "    activity_regularizer=l2(0.002),\n",
    "    kernel_initializer='VarianceScaling',\n",
    "    input_shape=X_train.shape[1:]\n",
    "))      \n",
    "model.add(MaxPooling2D(pool_size=2, padding=\"same\"))\n",
    "\n",
    "# second conv stack\n",
    "model.add(Conv2D(\n",
    "    filters=128, \n",
    "    kernel_size=5, \n",
    "    strides=1, \n",
    "    padding=\"same\",\n",
    "    activation=\"relu\",\n",
    "    activity_regularizer=l2(0.002),\n",
    "    kernel_initializer='VarianceScaling'\n",
    "))     \n",
    "model.add(MaxPooling2D(pool_size=2, padding=\"same\"))  \n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "# dense layers\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64, activation=\"relu\"))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "model.add(Dense(128, activation=\"relu\"))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "model.add(Dense(10, activation=\"softmax\"))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select Loss Function, Optimizer, and Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZHek_bnng3xp"
   },
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1091
    },
    "colab_type": "code",
    "id": "Hc_E0cTog96t",
    "outputId": "4fb76b2e-08bc-4a38-d574-79490df067f5"
   },
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    batch_size=50,\n",
    "    epochs=10,\n",
    "    validation_data=(X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_histories(history):\n",
    "    plt.plot(history.history['acc'], label='train')\n",
    "    plt.plot(history.history['val_acc'], label='test')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.title('Learning Curves')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    plt.plot(history.history['loss'], label='train')\n",
    "    plt.plot(history.history['val_loss'], label='test')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.title('Loss Function')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    return None\n",
    "\n",
    "plot_histories(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "N1Rp9FTYhBpZ",
    "outputId": "8ac2e5be-314a-4284-ea31-ddbfeda1d4de"
   },
   "outputs": [],
   "source": [
    "#train and test loss and scores respectively\n",
    "train_scores = model.evaluate(X_train, y_train)\n",
    "test_scores = model.evaluate(X_test, y_test)\n",
    "\n",
    "print('\\nTraining Loss:', train_scores[0])\n",
    "print('Training Accuracy:', train_scores[1])\n",
    "print('\\nTest Loss:', test_scores[0])\n",
    "print('Test Accuracy:', test_scores[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTES**\n",
    "\n",
    "The best-reported cross-validated test accuracy I found was 83% (from [this paper](https://arxiv.org/pdf/1808.08405.pdf)), albeit my search was non-exaustive. They used data augmentation. \n",
    "\n",
    "An even better data augmentation technique to try may be the one described [here](https://ai.googleblog.com/2019/04/specaugment-new-data-augmentation.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import svm, datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(\n",
    "        y_true,\n",
    "        y_pred,\n",
    "        labels=None,\n",
    "        normalize=False,\n",
    "        title=None,\n",
    "        cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "\n",
    "    if not title:\n",
    "        if normalize:\n",
    "            title = 'Normalized confusion matrix'\n",
    "        else:\n",
    "            title = 'Confusion matrix, without normalization'\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    y_true = y_true.argmax(axis=1)\n",
    "    y_pred = y_pred.argmax(axis=1)\n",
    "\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "    # Only use the labels that appear in the data\n",
    "    #classes = classes[unique_labels(y_true, y_pred)]\n",
    "    ul = unique_labels(y_true, y_pred)\n",
    "    ul = ul.tolist()\n",
    "    classes = []\n",
    "    for label in ul:\n",
    "        classes.append(labels[label])\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(8, 8))\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    # ax.figure.colorbar(im, ax=ax)\n",
    "\n",
    "    # We want to show all ticks...\n",
    "    ax.set(xticks=np.arange(cm.shape[1]),\n",
    "           yticks=np.arange(cm.shape[0]),\n",
    "           # ... and label them with the respective list entries\n",
    "           xticklabels=classes, yticklabels=classes,\n",
    "           title=title,\n",
    "           ylabel='True label',\n",
    "           xlabel='Predicted label')\n",
    "\n",
    "    # Rotate the tick labels and set their alignment.\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "             rotation_mode=\"anchor\")\n",
    "\n",
    "    # Loop over data dimensions and create text annotations.\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j, i, format(cm[i, j], fmt),\n",
    "                    ha=\"center\", va=\"center\",\n",
    "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    fig.tight_layout()\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    return ax\n",
    "\n",
    "\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "# obtain predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Plot normalized confusion matrix\n",
    "ax = plot_confusion_matrix(y_test, y_pred, labels=class_label_to_name, normalize=True,\n",
    "                      title='Normalized confusion matrix')\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Urban_data_preprocess.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
